{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "K-zfnhLJ2PIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "2cvnZ1xe1sbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "metadata": {
        "id": "51sg_zHp1zLX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2KQhnCsa1p-A"
      },
      "outputs": [],
      "source": [
        "def download_csv_from_google_drive(url):\n",
        "    \"\"\"\n",
        "    Downloads a CSV file from a Google Drive link and returns it as a pandas DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    url (str): The link to the Google Drive file shared publicly.\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: A DataFrame containing the data from the downloaded CSV file.\n",
        "    \"\"\"\n",
        "    # Construct the Google Drive direct download link\n",
        "    file_id = url.split('/')[-2]\n",
        "    direct_download_url = 'https://drive.google.com/uc?id=' + file_id\n",
        "\n",
        "    # Read the CSV file into a pandas DataFrame\n",
        "    df = pd.read_csv(direct_download_url)\n",
        "\n",
        "    return df\n",
        "\n",
        "def download_file_from_google_drive(url):\n",
        "    \"\"\"\n",
        "    Downloads a file from a publicly shared Google Drive link and returns it.\n",
        "\n",
        "    Parameters:\n",
        "    url (str): The link to the publicly shared Google Drive file.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Extract the file ID from the URL\n",
        "    file_id = url.split('/')[-2]\n",
        "\n",
        "    # Download the file using the gdown command\n",
        "    !gdown $file_id\n",
        "\n",
        "def search_and_download_images(categories, search_terms, num_images_per_category, base_dir):\n",
        "    \"\"\"\n",
        "    Search and download images using DuckDuckGo search.\n",
        "\n",
        "    Args:\n",
        "        categories (list): List of category names.\n",
        "        search_terms (list): List of search terms corresponding to each category.\n",
        "        num_images_per_category (int): Number of images to download per category.\n",
        "        base_dir (str): Base directory where images will be saved.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    Example Usage:\n",
        "    --------------\n",
        "    categories = [\"Grizzly Bear\", \"Black Bear\", \"Teddy Bear\"]\n",
        "    search_terms = [\"grizzly bear\", \"black bear\", \"teddy bear\"]\n",
        "    num_images_per_category = 50\n",
        "    base_dir = \"bears_dataset\"\n",
        "\n",
        "    search_and_download_images(categories, search_terms, num_images_per_category, base_dir)\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        from duckduckgo_search import ddg_images\n",
        "    except ImportError:\n",
        "        !pip install -q fastai duckduckgo_search\n",
        "        from duckduckgo_search import ddg_images\n",
        "\n",
        "    from fastai.vision.utils import download_images\n",
        "    from pathlib import Path\n",
        "    from fastcore.all import L\n",
        "\n",
        "    for category, search_term in zip(categories, search_terms):\n",
        "        folder_name = category.replace(\" \", \"_\").lower()\n",
        "        dest = Path(base_dir) / folder_name\n",
        "        dest.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        search_results = L(ddg_images(search_term, max_results=num_images_per_category)).itemgot('image')\n",
        "        download_images(dest, urls=search_results)\n",
        "\n",
        "        print(f\"Downloaded {num_images_per_category} images for '{search_term}' and saved to '{dest}'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YL4YQATDLO6P"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Checking"
      ],
      "metadata": {
        "id": "AKs-gbHi3ANp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_columns(df):\n",
        "    \"\"\"\n",
        "    Analyzes each column in a DataFrame by displaying the column name, the number of unique values,\n",
        "    and the count of each unique value in the column.\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): The DataFrame to be analyzed.\n",
        "    \"\"\"\n",
        "    for column_name in df.columns:\n",
        "        print(f'Column name: {column_name}, number of unique values: {df[column_name].nunique()}')\n",
        "        print(df[column_name].value_counts().sort_index())\n",
        "        print('-----------')\n",
        "def analyze_duplicates_and_missing_values(df):\n",
        "    \"\"\"\n",
        "    Analyzes a DataFrame for duplicated rows and missing values in columns.\n",
        "    Displays the total count of duplicated rows and a summary of columns with missing values.\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): The DataFrame to be analyzed.\n",
        "    \"\"\"\n",
        "    # Analyze duplicated rows\n",
        "    duplicated_count = df.duplicated().sum()\n",
        "    print(f'Total count of duplicated rows: {duplicated_count}')\n",
        "    print('----')\n",
        "\n",
        "    # Analyze missing values in columns\n",
        "    missing_values = df.isna().sum()\n",
        "    missing_values = missing_values[missing_values > 0]\n",
        "    if not missing_values.empty:\n",
        "        print(f'DataFrame length: {len(df)}\\nColumns with missing values: \\n{missing_values}')\n",
        "    else:\n",
        "        print('No columns with missing values found')\n",
        "\n"
      ],
      "metadata": {
        "id": "EYqSCp9A1sY7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Visualising"
      ],
      "metadata": {
        "id": "llejMu9s9A3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def correlation_matrix_with_phik(df, target=None, height=14, width=9):\n",
        "    \"\"\"\n",
        "    Generates a correlation matrix using the Phik library and displays it as a heatmap.\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): The DataFrame for which to generate the correlation matrix.\n",
        "    target (str): The name of the target column in the DataFrame (if specified).\n",
        "    height (int): Height of the heatmap figure.\n",
        "    width (int): Width of the heatmap figure.\n",
        "    \"\"\"\n",
        "    # Check if phik is already installed\n",
        "    try:\n",
        "        import phik\n",
        "    except ImportError:\n",
        "        !pip -q install phik\n",
        "\n",
        "    # Import necessary libraries\n",
        "    from phik.report import plot_correlation_matrix\n",
        "    import seaborn as sns\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Calculate the Phik correlation matrix\n",
        "    phik_overview = df.phik_matrix()\n",
        "\n",
        "    # Create a figure with a heatmap\n",
        "    plt.figure(figsize=(height, width))\n",
        "\n",
        "    # Customize the heatmap appearance\n",
        "    sns.heatmap(phik_overview, annot=True, fmt=\".2f\", annot_kws={\"size\": 8}, cmap=\"coolwarm\")\n",
        "\n",
        "    # If a target column is specified, display its correlation with other columns\n",
        "    if target is not None:\n",
        "        from phik import report\n",
        "        phik_sorted = phik_overview[target].sort_values(ascending=False)\n",
        "        print(f\"\\nCorrelation of '{target}' with other columns:\")\n",
        "        print(f'{phik_sorted}\\n')\n"
      ],
      "metadata": {
        "id": "YY4wgbCc9nuZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XuMohjqr1sPF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B7y-kmu61sM4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. 200.9\n",
        "13. Нельзя создать экземпляр абстрактного класса"
      ],
      "metadata": {
        "id": "abzc9tzxJI6j"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yKZkotT1K-5u"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}